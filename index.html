<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="EaOBAFJS2oZAhYIqcnG58x5_a5rmawbEfR2m7fBQRSk"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Astra-vision - Computer vision group, Astra, Inria</title> <meta name="author" content=" "/> <meta name="description" content="<strong>We study 2D vision and 3D perception for robust scene understanding.</strong> Our research focuses on relaxing the use of abundant data and supervision, stepping towards weak-/un-supervised vision algorithms, while providing models that are more interpretable. We primarily address autonomous driving but our research expands to a variety of indoor and outdoor applications." /> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>âš›ï¸</text></svg>"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://astra-vision.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">Home<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/team/">Team</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/jobs/">Jobs</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Astra-vision </h1> <p class="desc">Computer vision group of <a href="https://astra-team.github.io" target="_blank" rel="noopener noreferrer">Astra Team</a>, <a href="https://www.inria.fr/fr/centre-inria-de-paris" target="_blank" rel="noopener noreferrer">Inria Paris</a></p> </header> <article> <div class="profile align-center"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/astravision-2023_700p-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/astravision-2023_700p-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/astravision-2023_700p-1400.webp"></source> <img src="/assets/img/astravision-2023_700p.jpg" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="astravision-2023_700p.jpg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="profile imgdate">Oct. 2023</div> </div> <div class="team-description"> <strong>We study 2D vision and 3D perception for robust scene understanding.</strong> Our research focuses on relaxing the use of abundant data and supervision, stepping towards weak-/un-supervised vision algorithms, while providing models that are more interpretable. We primarily address autonomous driving but our research expands to a variety of indoor and outdoor applications. </div> <div class="research"> <h2>Research axes</h2> <div class="axes"> <article><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/research/axis_weak-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/research/axis_weak-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/research/axis_weak-1400.webp"></source> <img src="/assets/img/research/axis_weak.png" class="axis" width="auto" height="auto" alt="Learning with less supervision" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="title">Learning with less supervision</div> </article> <article><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/research/axis_complexvision-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/research/axis_complexvision-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/research/axis_complexvision-1400.webp"></source> <img src="/assets/img/research/axis_complexvision.png" class="axis" width="auto" height="auto" alt="Vision in complex conditions" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="title">Vision in complex conditions</div> </article> <article><figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/research/axis_3d-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/research/axis_3d-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/research/axis_3d-1400.webp"></source> <img src="/assets/img/research/axis_3d.png" class="axis" width="auto" height="auto" alt="3D scene understanding" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="title">3D scene understanding</div> </article> </div> </div> <div class="news"> <h2>news</h2> <div class="table-responsive" style="max-height: 20vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 13, 2025</th> <td> Organizing the 2nd <a href="https://www.acvss.ai" target="_blank" rel="noopener noreferrer">African Computer Vision Summer School</a> in Kigali, Rwanda from July 13th to 23rd, with an amazing lineup of speakers. </td> </tr> <tr> <th scope="row">Jun 27, 2025</th> <td> <a href="https://arxiv.org/abs/2502.07784" target="_blank" rel="noopener noreferrer">MatSwap</a> was awarded ğŸ† <strong>Best Paper Honorable Mention</strong> at <a href="https://conferences.eg.org/egsr2025/" target="_blank" rel="noopener noreferrer">EGSR 25</a>. Kudos to <a href="https://ivnlps.github.io/" target="_blank" rel="noopener noreferrer">Ivan</a> and team. </td> </tr> <tr> <th scope="row">Jun 12, 2025</th> <td> Organizing the <a href="https://sites.google.com/view/pixfoundation/" target="_blank" rel="noopener noreferrer">PixFoundation</a> workshop on Pixel-level Vision Foundation Models at <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2025</a>, in Nashville, USA. </td> </tr> <tr> <th scope="row">Jun 2, 2025</th> <td> Happy to host <a href="https://fabvio.github.io/" target="_blank" rel="noopener noreferrer">Fabio Pizzati</a>, postdoc at MBZUAI, for a one month visit. Welcome Fabio ğŸ‘‹ğŸ˜ƒ </td> </tr> <tr> <th scope="row">Apr 25, 2025</th> <td> 6 group members received a CVPR 25 Outstanding Reviewer Award. With <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-outstanding-reviewer" target="_blank" rel="noopener noreferrer">6 outstanding reviewers</a> (bravo to Alexandre, Gilles, Ivan, Quan, Mohammad, Yasser) and <a href="https://cvpr.thecvf.com/Conferences/2025/ProgramCommittee#all-area-chair" target="_blank" rel="noopener noreferrer">3 Area Chairs</a> (Andrei, Raoul, Renaud), Astra-Vision is making its mark to CVPR. </td> </tr> <tr> <th scope="row">Apr 15, 2025</th> <td> <a href="https://astra-vision.github.io/LiDPM/">LiDPM</a> is accepted as oral (approx. 7%) at <a href="https://ieee-iv.org/2025/" target="_blank" rel="noopener noreferrer">IV 2025</a>. Kudos to <a href="https://t-martyniuk.github.io/" target="_blank" rel="noopener noreferrer">Tetiana</a>. </td> </tr> <tr> <th scope="row">Apr 15, 2025</th> <td> ğŸ“ We opened a PhD topic on â€œPhysics-Grounded Vision Foundation Modelsâ€. Apply <strong>before May 20th 2025</strong>. More details on the <a href="/jobs">job page</a>. </td> </tr> <tr> <th scope="row">Apr 15, 2025</th> <td> Our paper <a href="https://arxiv.org/abs/2504.10487" target="_blank" rel="noopener noreferrer">FLOSS</a> is out with <a href="https://github.com/yasserben/FLOSS" target="_blank" rel="noopener noreferrer">code</a>. It builds on the intruiging observations that some CLIP templates excel at given semantic segmentation classes. FLOSS formulates a new training-free Open-vocabulary Semantic Segmentation that can be plugged into any methods. Bravo <a href="https://yasserben.github.io/" target="_blank" rel="noopener noreferrer">Yasser</a> and team. </td> </tr> <tr> <th scope="row">Feb 1, 2025</th> <td> Welcoming two new interns in our group: <a href="https://www.linkedin.com/in/fatima-balde-146312220/" target="_blank" rel="noopener noreferrer">Fatima Balde</a> and <a href="https://www.linkedin.com/in/jonathan-seele-913691253/" target="_blank" rel="noopener noreferrer">Jonathan Seele</a>. Welcome and good luck for your work. </td> </tr> <tr> <th scope="row">Oct 20, 2024</th> <td> New papers building on VLMs: ğŸ“ <a href="https://arxiv.org/abs/2410.05270" target="_blank" rel="noopener noreferrer">ProLIP</a> shows that adequately finetuning the last CLIP layer significantly boost few-shot classification. â˜• <a href="https://arxiv.org/abs/2410.08211" target="_blank" rel="noopener noreferrer">LatteCLIP</a> proposes an unsupervised prototype-based CLIP finetuning from only synthetic labels. </td> </tr> <tr> <th scope="row">Oct 1, 2024</th> <td> The group organized the first <a href="https://www.acvss.ai" target="_blank" rel="noopener noreferrer">ACVSS summer school</a> in Kenya and the <a href="https://wscv-indaba.github.io/2024" target="_blank" rel="noopener noreferrer">3rd WSCV workshop</a> in Deep Learning Indaba in Dakar. </td> </tr> <tr> <th scope="row">Jul 1, 2024</th> <td> ğŸ§  <a href="https://weihaox.github.io/UMBRAE/" target="_blank" rel="noopener noreferrer">UMBRAE: Unified Brain Decoding</a> is accepted to ECCV 2024. </td> </tr> <tr> <th scope="row">May 25, 2024</th> <td> ğŸ•µï¸ <a href="https://astra-vision.github.io/PaSCo/">PaSCo</a> selected as Best Paper Award Candidate of <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2024</a>. </td> </tr> <tr> <th scope="row">Feb 26, 2024</th> <td> Our 3 papers got accepted to <a href="https://cvpr.thecvf.com/" target="_blank" rel="noopener noreferrer">CVPR 2024</a>: ğŸ¨ <a href="https://astra-vision.github.io/MaterialPalette/">Material Palette</a>, ğŸ´ <a href="https://github.com/astra-vision/FAMix" target="_blank" rel="noopener noreferrer">FAMiX</a>, ğŸ•µï¸ <a href="https://astra-vision.github.io/PaSCo/">PaSCo</a>. </td> </tr> <tr> <th scope="row">Dec 6, 2023</th> <td> We just released three new works: ğŸ¨ <a href="https://astra-vision.github.io/MaterialPalette/">Material Palette</a> for extraction of materials from a single image, ğŸ´ <a href="https://github.com/astra-vision/FAMix" target="_blank" rel="noopener noreferrer">FAMiX</a> a simple recipe for domain generalized segmentation, ğŸ•µï¸ <a href="https://astra-vision.github.io/PaSCo/">PaSCo</a> the first 3D panoptic scene completion with uncertainty. Joint works with Oxford Uni., Valeo.ai, and TUM. </td> </tr> <tr> <th scope="row">Dec 6, 2023</th> <td> Opening of the internship seasons, check the <a href="/jobs">job page</a>. (18/12: ğŸ”Š new internships added) </td> </tr> <tr> <th scope="row">Oct 11, 2023</th> <td> Our collaborative work ğŸ§  <a href="https://weihaox.github.io/DREAM/" target="_blank" rel="noopener noreferrer">DREAM</a> on visual decoding of brain activity is accepted in <a href="https://wacv2024.thecvf.com/" target="_blank" rel="noopener noreferrer">WACV 2024</a>. </td> </tr> <tr> <th scope="row">Oct 10, 2023</th> <td> Intense week at ICCV23 with 2 papers (<a href="https://astra-vision.github.io/PODA/" target="_blank">PÃ˜DA</a> and <a href="https://astra-vision.github.io/SceneRF/" target="_blank">SceneRF</a>), 1 workshop keynote (R. de Charette @<a href="https://valeoai.github.io/bravo/#speakers" target="_blank" rel="noopener noreferrer">BRAVO</a>), and 1 cool new group picture. Check <a href="https://www.linkedin.com/posts/raoul-de-charette_iccv-2023-was-intense-full-of-inspiring-activity-7117540108232204288-m3O4?utm_source=share&amp;utm_medium=member_desktop" target="_blank" rel="noopener noreferrer">this post</a> for more info. </td> </tr> <tr> <th scope="row">Sep 8, 2023</th> <td> We organize the 2nd <a href="https://wscv-indaba.github.io" target="_blank" rel="noopener noreferrer">Weakly Supervised Computer Vision workshop</a> at the 2023 Deep Learning Indaba in Accra, Ghana. </td> </tr> <tr> <th scope="row">Jul 15, 2023</th> <td> <img class="emoji" title=":loudspeaker:" alt=":loudspeaker:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f4e2.png" height="20" width="20"> Two papers accepted for ICCV 2023: <a href="https://astra-vision.github.io/PODA/" target="_blank">PÃ˜DA</a> and <a href="https://astra-vision.github.io/SceneRF/" target="_blank">SceneRF</a>. </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> Â© Copyright 2025 . Uses <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-FE57SY17V6"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-FE57SY17V6");</script> </body> </html>